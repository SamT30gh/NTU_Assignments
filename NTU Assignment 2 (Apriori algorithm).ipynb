{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e8d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets: \n",
      "    support               itemsets\n",
      "0  0.800000                (Bread)\n",
      "1  0.400000               (Butter)\n",
      "2  0.466667                (Juice)\n",
      "3  0.600000                 (Milk)\n",
      "4  0.400000        (Bread, Butter)\n",
      "5  0.266667         (Juice, Bread)\n",
      "6  0.400000          (Milk, Bread)\n",
      "7  0.200000         (Milk, Butter)\n",
      "8  0.266667          (Juice, Milk)\n",
      "9  0.200000  (Milk, Bread, Butter)\n",
      "\n",
      "Association Rules: \n",
      "      antecedents consequents  antecedent support  consequent support  \\\n",
      "0        (Butter)     (Bread)                 0.4                 0.8   \n",
      "1          (Milk)     (Bread)                 0.6                 0.8   \n",
      "2  (Milk, Butter)     (Bread)                 0.2                 0.8   \n",
      "\n",
      "   support  confidence      lift  leverage  conviction  zhangs_metric  \n",
      "0      0.4    1.000000  1.250000      0.08         inf       0.333333  \n",
      "1      0.4    0.666667  0.833333     -0.08         0.6      -0.333333  \n",
      "2      0.2    1.000000  1.250000      0.04         inf       0.250000  \n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 49.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "data = [['Milk', 'Bread', 'Butter'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Milk', 'Bread', 'Butter'],\n",
    "        ['Milk', 'Bread'],\n",
    "        ['Milk', 'Juice'],\n",
    "        ['Bread', 'Juice'],\n",
    "        ['Milk', 'Bread', 'Juice'],\n",
    "        ['Milk', 'Bread', 'Butter'],\n",
    "        ['Milk', 'Juice'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Milk', 'Juice'],\n",
    "        ['Bread', 'Juice'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Milk', 'Bread'],\n",
    "        ['Bread', 'Juice']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets: \")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules: \")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863704ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets with Varying Unique Items: \n",
      "    support                  itemsets\n",
      "0  0.933333                  (Item_1)\n",
      "1  0.600000                  (Item_2)\n",
      "2  0.533333                  (Item_3)\n",
      "3  0.533333          (Item_1, Item_2)\n",
      "4  0.533333          (Item_1, Item_3)\n",
      "5  0.400000          (Item_3, Item_2)\n",
      "6  0.400000  (Item_1, Item_3, Item_2)\n",
      "\n",
      "Association Rules with Varying Unique Items: \n",
      "        antecedents       consequents  antecedent support  consequent support  \\\n",
      "0          (Item_2)          (Item_1)            0.600000            0.933333   \n",
      "1          (Item_3)          (Item_1)            0.533333            0.933333   \n",
      "2          (Item_3)          (Item_2)            0.533333            0.600000   \n",
      "3          (Item_2)          (Item_3)            0.600000            0.533333   \n",
      "4  (Item_1, Item_3)          (Item_2)            0.533333            0.600000   \n",
      "5  (Item_1, Item_2)          (Item_3)            0.533333            0.533333   \n",
      "6  (Item_3, Item_2)          (Item_1)            0.400000            0.933333   \n",
      "7          (Item_3)  (Item_1, Item_2)            0.533333            0.533333   \n",
      "8          (Item_2)  (Item_1, Item_3)            0.600000            0.533333   \n",
      "\n",
      "    support  confidence      lift  leverage  conviction  zhangs_metric  \n",
      "0  0.533333    0.888889  0.952381 -0.026667    0.600000      -0.111111  \n",
      "1  0.533333    1.000000  1.071429  0.035556         inf       0.142857  \n",
      "2  0.400000    0.750000  1.250000  0.080000    1.600000       0.428571  \n",
      "3  0.400000    0.666667  1.250000  0.080000    1.400000       0.500000  \n",
      "4  0.400000    0.750000  1.250000  0.080000    1.600000       0.428571  \n",
      "5  0.400000    0.750000  1.406250  0.115556    1.866667       0.619048  \n",
      "6  0.400000    1.000000  1.071429  0.026667         inf       0.111111  \n",
      "7  0.400000    0.750000  1.406250  0.115556    1.866667       0.619048  \n",
      "8  0.400000    0.666667  1.250000  0.080000    1.400000       0.500000  \n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 47.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate transactions with varying unique items\n",
    "def generate_transactions(num_transactions, max_items):\n",
    "    items = ['Item_' + str(i) for i in range(1, max_items + 1)]\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        num_items = random.randint(1, max_items)\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Generate transactions with varying unique items\n",
    "num_transactions = 15  # same as the previous example\n",
    "max_items = 3  # change the number of unique items here\n",
    "data_varying = generate_transactions(num_transactions, max_items)\n",
    "\n",
    "# Rest of the code remains the same as the previous example\n",
    "te_varying = TransactionEncoder()\n",
    "te_ary_varying = te_varying.fit(data_varying).transform(data_varying)\n",
    "df_varying = pd.DataFrame(te_ary_varying, columns=te_varying.columns_)\n",
    "\n",
    "frequent_itemsets_varying = apriori(df_varying, min_support=0.2, use_colnames=True)\n",
    "rules_varying = association_rules(frequent_itemsets_varying, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets with Varying Unique Items: \")\n",
    "print(frequent_itemsets_varying)\n",
    "print(\"\\nAssociation Rules with Varying Unique Items: \")\n",
    "print(rules_varying)\n",
    "#By changing the value of max_items, you can generate a dataset with a different number of unique items. This modified code will still utilize the mlxtend library and produce the frequent itemsets and association rules for the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb41bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets with Varying Unique Items: \n",
      "      support                                           itemsets\n",
      "0    0.600000                                           (Item_1)\n",
      "1    0.600000                                           (Item_2)\n",
      "2    0.533333                                           (Item_3)\n",
      "3    0.600000                                           (Item_4)\n",
      "4    0.800000                                           (Item_5)\n",
      "..        ...                                                ...\n",
      "250  0.200000  (Item_1, Item_6, Item_7, Item_3, Item_2, Item_...\n",
      "251  0.266667  (Item_1, Item_6, Item_7, Item_4, Item_2, Item_...\n",
      "252  0.266667  (Item_1, Item_6, Item_7, Item_3, Item_4, Item_...\n",
      "253  0.200000  (Item_6, Item_7, Item_3, Item_4, Item_2, Item_...\n",
      "254  0.200000  (Item_1, Item_6, Item_7, Item_3, Item_4, Item_...\n",
      "\n",
      "[255 rows x 2 columns]\n",
      "\n",
      "Association Rules with Varying Unique Items: \n",
      "           antecedents                                       consequents  \\\n",
      "0             (Item_1)                                          (Item_2)   \n",
      "1             (Item_2)                                          (Item_1)   \n",
      "2             (Item_3)                                          (Item_1)   \n",
      "3             (Item_1)                                          (Item_4)   \n",
      "4             (Item_4)                                          (Item_1)   \n",
      "...                ...                                               ...   \n",
      "4109  (Item_6, Item_2)  (Item_1, Item_7, Item_3, Item_4, Item_8, Item_5)   \n",
      "4110  (Item_6, Item_8)  (Item_1, Item_7, Item_3, Item_4, Item_2, Item_5)   \n",
      "4111  (Item_3, Item_2)  (Item_1, Item_6, Item_7, Item_4, Item_8, Item_5)   \n",
      "4112  (Item_4, Item_2)  (Item_1, Item_6, Item_7, Item_3, Item_8, Item_5)   \n",
      "4113  (Item_8, Item_2)  (Item_1, Item_6, Item_7, Item_3, Item_4, Item_5)   \n",
      "\n",
      "      antecedent support  consequent support   support  confidence      lift  \\\n",
      "0               0.600000            0.600000  0.400000    0.666667  1.111111   \n",
      "1               0.600000            0.600000  0.400000    0.666667  1.111111   \n",
      "2               0.533333            0.600000  0.333333    0.625000  1.041667   \n",
      "3               0.600000            0.600000  0.466667    0.777778  1.296296   \n",
      "4               0.600000            0.600000  0.466667    0.777778  1.296296   \n",
      "...                  ...                 ...       ...         ...       ...   \n",
      "4109            0.333333            0.266667  0.200000    0.600000  2.250000   \n",
      "4110            0.333333            0.266667  0.200000    0.600000  2.250000   \n",
      "4111            0.333333            0.333333  0.200000    0.600000  1.800000   \n",
      "4112            0.333333            0.266667  0.200000    0.600000  2.250000   \n",
      "4113            0.333333            0.266667  0.200000    0.600000  2.250000   \n",
      "\n",
      "      leverage  conviction  zhangs_metric  \n",
      "0     0.040000    1.200000       0.250000  \n",
      "1     0.040000    1.200000       0.250000  \n",
      "2     0.013333    1.066667       0.085714  \n",
      "3     0.106667    1.800000       0.571429  \n",
      "4     0.106667    1.800000       0.571429  \n",
      "...        ...         ...            ...  \n",
      "4109  0.111111    1.833333       0.833333  \n",
      "4110  0.111111    1.833333       0.833333  \n",
      "4111  0.088889    1.666667       0.666667  \n",
      "4112  0.111111    1.833333       0.833333  \n",
      "4113  0.111111    1.833333       0.833333  \n",
      "\n",
      "[4114 rows x 10 columns]\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 70.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate transactions with varying unique items\n",
    "def generate_transactions(num_transactions, max_items):\n",
    "    items = ['Item_' + str(i) for i in range(1, max_items + 1)]\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        num_items = random.randint(1, max_items)\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Generate transactions with varying unique items\n",
    "num_transactions = 15  # same as the previous example\n",
    "max_items = 8  # change the number of unique items here\n",
    "data_varying = generate_transactions(num_transactions, max_items)\n",
    "\n",
    "# Rest of the code remains the same as the previous example\n",
    "te_varying = TransactionEncoder()\n",
    "te_ary_varying = te_varying.fit(data_varying).transform(data_varying)\n",
    "df_varying = pd.DataFrame(te_ary_varying, columns=te_varying.columns_)\n",
    "\n",
    "frequent_itemsets_varying = apriori(df_varying, min_support=0.2, use_colnames=True)\n",
    "rules_varying = association_rules(frequent_itemsets_varying, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets with Varying Unique Items: \")\n",
    "print(frequent_itemsets_varying)\n",
    "print(\"\\nAssociation Rules with Varying Unique Items: \")\n",
    "print(rules_varying)\n",
    "#By changing the value of max_items, you can generate a dataset with a different number of unique items. This modified code will still utilize the mlxtend library and produce the frequent itemsets and association rules for the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079d53e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets with Varying Unique Items: \n",
      "       support                                           itemsets\n",
      "0     0.666667                                           (Item_1)\n",
      "1     0.800000                                          (Item_10)\n",
      "2     0.666667                                           (Item_2)\n",
      "3     0.733333                                           (Item_3)\n",
      "4     0.733333                                           (Item_4)\n",
      "...        ...                                                ...\n",
      "1018  0.200000  (Item_1, Item_6, Item_7, Item_9, Item_4, Item_...\n",
      "1019  0.200000  (Item_1, Item_6, Item_7, Item_9, Item_3, Item_...\n",
      "1020  0.200000  (Item_1, Item_6, Item_7, Item_9, Item_3, Item_...\n",
      "1021  0.266667  (Item_6, Item_7, Item_9, Item_3, Item_4, Item_...\n",
      "1022  0.200000  (Item_1, Item_6, Item_7, Item_9, Item_3, Item_...\n",
      "\n",
      "[1023 rows x 2 columns]\n",
      "\n",
      "Association Rules with Varying Unique Items: \n",
      "            antecedents                                        consequents  \\\n",
      "0              (Item_1)                                          (Item_10)   \n",
      "1             (Item_10)                                           (Item_1)   \n",
      "2              (Item_1)                                           (Item_2)   \n",
      "3              (Item_2)                                           (Item_1)   \n",
      "4              (Item_1)                                           (Item_3)   \n",
      "...                 ...                                                ...   \n",
      "39815  (Item_6, Item_7)  (Item_1, Item_9, Item_3, Item_4, Item_2, Item_...   \n",
      "39816  (Item_6, Item_9)  (Item_1, Item_7, Item_3, Item_4, Item_2, Item_...   \n",
      "39817  (Item_6, Item_2)  (Item_1, Item_7, Item_9, Item_3, Item_4, Item_...   \n",
      "39818  (Item_6, Item_8)  (Item_1, Item_7, Item_9, Item_3, Item_4, Item_...   \n",
      "39819  (Item_6, Item_5)  (Item_1, Item_7, Item_9, Item_3, Item_4, Item_...   \n",
      "\n",
      "       antecedent support  consequent support   support  confidence      lift  \\\n",
      "0                0.666667            0.800000  0.600000        0.90  1.125000   \n",
      "1                0.800000            0.666667  0.600000        0.75  1.125000   \n",
      "2                0.666667            0.666667  0.533333        0.80  1.200000   \n",
      "3                0.666667            0.666667  0.533333        0.80  1.200000   \n",
      "4                0.666667            0.733333  0.533333        0.80  1.090909   \n",
      "...                   ...                 ...       ...         ...       ...   \n",
      "39815            0.333333            0.266667  0.200000        0.60  2.250000   \n",
      "39816            0.266667            0.266667  0.200000        0.75  2.812500   \n",
      "39817            0.333333            0.266667  0.200000        0.60  2.250000   \n",
      "39818            0.333333            0.266667  0.200000        0.60  2.250000   \n",
      "39819            0.333333            0.266667  0.200000        0.60  2.250000   \n",
      "\n",
      "       leverage  conviction  zhangs_metric  \n",
      "0      0.066667    2.000000       0.333333  \n",
      "1      0.066667    1.333333       0.555556  \n",
      "2      0.088889    1.666667       0.500000  \n",
      "3      0.088889    1.666667       0.500000  \n",
      "4      0.044444    1.333333       0.250000  \n",
      "...         ...         ...            ...  \n",
      "39815  0.111111    1.833333       0.833333  \n",
      "39816  0.128889    2.933333       0.878788  \n",
      "39817  0.111111    1.833333       0.833333  \n",
      "39818  0.111111    1.833333       0.833333  \n",
      "39819  0.111111    1.833333       0.833333  \n",
      "\n",
      "[39820 rows x 10 columns]\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 561 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate transactions with varying unique items\n",
    "def generate_transactions(num_transactions, max_items):\n",
    "    items = ['Item_' + str(i) for i in range(1, max_items + 1)]\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        num_items = random.randint(1, max_items)\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Generate transactions with varying unique items\n",
    "num_transactions = 15  # same as the previous example\n",
    "max_items = 10  # change the number of unique items here\n",
    "data_varying = generate_transactions(num_transactions, max_items)\n",
    "\n",
    "# Rest of the code remains the same as the previous example\n",
    "te_varying = TransactionEncoder()\n",
    "te_ary_varying = te_varying.fit(data_varying).transform(data_varying)\n",
    "df_varying = pd.DataFrame(te_ary_varying, columns=te_varying.columns_)\n",
    "\n",
    "frequent_itemsets_varying = apriori(df_varying, min_support=0.2, use_colnames=True)\n",
    "rules_varying = association_rules(frequent_itemsets_varying, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets with Varying Unique Items: \")\n",
    "print(frequent_itemsets_varying)\n",
    "print(\"\\nAssociation Rules with Varying Unique Items: \")\n",
    "print(rules_varying)\n",
    "#By changing the value of max_items, you can generate a dataset with a different number of unique items. This modified code will still utilize the mlxtend library and produce the frequent itemsets and association rules for the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4fe281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "      support             itemsets\n",
      "0   0.533333             (Action)\n",
      "1   0.266667          (Adventure)\n",
      "2   0.666667              (Drama)\n",
      "3   0.600000            (Romance)\n",
      "4   0.333333             (Sci-Fi)\n",
      "5   0.266667  (Action, Adventure)\n",
      "6   0.200000      (Drama, Action)\n",
      "7   0.200000    (Romance, Action)\n",
      "8   0.266667     (Sci-Fi, Action)\n",
      "9   0.466667     (Drama, Romance)\n",
      "10  0.200000      (Drama, Sci-Fi)\n",
      "\n",
      "Association Rules:\n",
      "    antecedents consequents  antecedent support  consequent support   support  \\\n",
      "0  (Adventure)    (Action)            0.266667            0.533333  0.266667   \n",
      "1     (Sci-Fi)    (Action)            0.333333            0.533333  0.266667   \n",
      "2      (Drama)   (Romance)            0.666667            0.600000  0.466667   \n",
      "3    (Romance)     (Drama)            0.600000            0.666667  0.466667   \n",
      "4     (Sci-Fi)     (Drama)            0.333333            0.666667  0.200000   \n",
      "\n",
      "   confidence      lift  leverage  conviction  zhangs_metric  \n",
      "0    1.000000  1.875000  0.124444         inf       0.636364  \n",
      "1    0.800000  1.500000  0.088889    2.333333       0.500000  \n",
      "2    0.700000  1.166667  0.066667    1.333333       0.428571  \n",
      "3    0.777778  1.166667  0.066667    1.500000       0.357143  \n",
      "4    0.600000  0.900000 -0.022222    0.833333      -0.142857  \n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    ['Action', 'Adventure', 'Sci-Fi'],\n",
    "    ['Action', 'Drama', 'Romance'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Adventure', 'Drama', 'Sci-Fi'],\n",
    "    ['Action', 'Romance'],\n",
    "    ['Drama', 'Sci-Fi'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Adventure', 'Romance'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Drama', 'Sci-Fi'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Sci-Fi'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Adventure'],\n",
    "    ['Drama', 'Romance']\n",
    "]\n",
    "\n",
    "# Convert the dataset to a one-hot encoded DataFrame\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Applying the Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generating association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# Printing the rules\n",
    "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\\n\", rules)\n",
    "#This code will help you find frequent itemsets and generate association rules based on the minimum support and confidence thresholds. Adjust the min_support and min_threshold parameters based on your dataset and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a7ab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 10 transactions:\n",
      "Frequent Itemsets:\n",
      "     support                                     itemsets\n",
      "0       1.0                                     (Action)\n",
      "1       1.0                                  (Adventure)\n",
      "2       1.0                                      (Drama)\n",
      "3       1.0                                    (Romance)\n",
      "4       1.0                                     (Sci-Fi)\n",
      "5       1.0                          (Action, Adventure)\n",
      "6       1.0                              (Drama, Action)\n",
      "7       1.0                            (Romance, Action)\n",
      "8       1.0                             (Sci-Fi, Action)\n",
      "9       1.0                           (Drama, Adventure)\n",
      "10      1.0                         (Romance, Adventure)\n",
      "11      1.0                          (Sci-Fi, Adventure)\n",
      "12      1.0                             (Drama, Romance)\n",
      "13      1.0                              (Drama, Sci-Fi)\n",
      "14      1.0                            (Romance, Sci-Fi)\n",
      "15      1.0                   (Drama, Action, Adventure)\n",
      "16      1.0                 (Romance, Action, Adventure)\n",
      "17      1.0                  (Sci-Fi, Action, Adventure)\n",
      "18      1.0                     (Drama, Romance, Action)\n",
      "19      1.0                      (Drama, Sci-Fi, Action)\n",
      "20      1.0                    (Romance, Sci-Fi, Action)\n",
      "21      1.0                  (Drama, Romance, Adventure)\n",
      "22      1.0                   (Drama, Sci-Fi, Adventure)\n",
      "23      1.0                 (Romance, Sci-Fi, Adventure)\n",
      "24      1.0                     (Drama, Romance, Sci-Fi)\n",
      "25      1.0          (Drama, Romance, Action, Adventure)\n",
      "26      1.0           (Drama, Sci-Fi, Action, Adventure)\n",
      "27      1.0         (Romance, Sci-Fi, Action, Adventure)\n",
      "28      1.0             (Drama, Romance, Sci-Fi, Action)\n",
      "29      1.0          (Drama, Romance, Sci-Fi, Adventure)\n",
      "30      1.0  (Sci-Fi, Adventure, Drama, Romance, Action)\n",
      "\n",
      "Association Rules:\n",
      "      antecedents                           consequents  antecedent support  \\\n",
      "0       (Action)                           (Adventure)                 1.0   \n",
      "1    (Adventure)                              (Action)                 1.0   \n",
      "2        (Drama)                              (Action)                 1.0   \n",
      "3       (Action)                               (Drama)                 1.0   \n",
      "4      (Romance)                              (Action)                 1.0   \n",
      "..           ...                                   ...                 ...   \n",
      "175     (Sci-Fi)   (Drama, Romance, Action, Adventure)                 1.0   \n",
      "176  (Adventure)      (Drama, Action, Romance, Sci-Fi)                 1.0   \n",
      "177      (Drama)  (Action, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "178    (Romance)    (Drama, Action, Sci-Fi, Adventure)                 1.0   \n",
      "179     (Action)   (Drama, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "\n",
      "     consequent support  support  confidence  lift  leverage  conviction  \\\n",
      "0                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "1                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "2                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "3                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "4                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "..                  ...      ...         ...   ...       ...         ...   \n",
      "175                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "176                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "177                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "178                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "179                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "\n",
      "     zhangs_metric  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "..             ...  \n",
      "175            0.0  \n",
      "176            0.0  \n",
      "177            0.0  \n",
      "178            0.0  \n",
      "179            0.0  \n",
      "\n",
      "[180 rows x 10 columns]\n",
      "\n",
      "\n",
      "Results for 15 transactions:\n",
      "Frequent Itemsets:\n",
      "     support                                     itemsets\n",
      "0       1.0                                     (Action)\n",
      "1       1.0                                  (Adventure)\n",
      "2       1.0                                      (Drama)\n",
      "3       1.0                                    (Romance)\n",
      "4       1.0                                     (Sci-Fi)\n",
      "5       1.0                          (Action, Adventure)\n",
      "6       1.0                              (Drama, Action)\n",
      "7       1.0                            (Romance, Action)\n",
      "8       1.0                             (Sci-Fi, Action)\n",
      "9       1.0                           (Drama, Adventure)\n",
      "10      1.0                         (Romance, Adventure)\n",
      "11      1.0                          (Sci-Fi, Adventure)\n",
      "12      1.0                             (Drama, Romance)\n",
      "13      1.0                              (Drama, Sci-Fi)\n",
      "14      1.0                            (Romance, Sci-Fi)\n",
      "15      1.0                   (Drama, Action, Adventure)\n",
      "16      1.0                 (Romance, Action, Adventure)\n",
      "17      1.0                  (Sci-Fi, Action, Adventure)\n",
      "18      1.0                     (Drama, Romance, Action)\n",
      "19      1.0                      (Drama, Sci-Fi, Action)\n",
      "20      1.0                    (Romance, Sci-Fi, Action)\n",
      "21      1.0                  (Drama, Romance, Adventure)\n",
      "22      1.0                   (Drama, Sci-Fi, Adventure)\n",
      "23      1.0                 (Romance, Sci-Fi, Adventure)\n",
      "24      1.0                     (Drama, Romance, Sci-Fi)\n",
      "25      1.0          (Drama, Romance, Action, Adventure)\n",
      "26      1.0           (Drama, Sci-Fi, Action, Adventure)\n",
      "27      1.0         (Romance, Sci-Fi, Action, Adventure)\n",
      "28      1.0             (Drama, Romance, Sci-Fi, Action)\n",
      "29      1.0          (Drama, Romance, Sci-Fi, Adventure)\n",
      "30      1.0  (Sci-Fi, Adventure, Drama, Romance, Action)\n",
      "\n",
      "Association Rules:\n",
      "      antecedents                           consequents  antecedent support  \\\n",
      "0       (Action)                           (Adventure)                 1.0   \n",
      "1    (Adventure)                              (Action)                 1.0   \n",
      "2        (Drama)                              (Action)                 1.0   \n",
      "3       (Action)                               (Drama)                 1.0   \n",
      "4      (Romance)                              (Action)                 1.0   \n",
      "..           ...                                   ...                 ...   \n",
      "175     (Sci-Fi)   (Drama, Romance, Action, Adventure)                 1.0   \n",
      "176  (Adventure)      (Drama, Action, Romance, Sci-Fi)                 1.0   \n",
      "177      (Drama)  (Action, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "178    (Romance)    (Drama, Action, Sci-Fi, Adventure)                 1.0   \n",
      "179     (Action)   (Drama, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "\n",
      "     consequent support  support  confidence  lift  leverage  conviction  \\\n",
      "0                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "1                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "2                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "3                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "4                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "..                  ...      ...         ...   ...       ...         ...   \n",
      "175                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "176                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "177                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "178                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "179                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "\n",
      "     zhangs_metric  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "..             ...  \n",
      "175            0.0  \n",
      "176            0.0  \n",
      "177            0.0  \n",
      "178            0.0  \n",
      "179            0.0  \n",
      "\n",
      "[180 rows x 10 columns]\n",
      "\n",
      "\n",
      "Results for 20 transactions:\n",
      "Frequent Itemsets:\n",
      "     support                                     itemsets\n",
      "0       1.0                                     (Action)\n",
      "1       1.0                                  (Adventure)\n",
      "2       1.0                                      (Drama)\n",
      "3       1.0                                    (Romance)\n",
      "4       1.0                                     (Sci-Fi)\n",
      "5       1.0                          (Action, Adventure)\n",
      "6       1.0                              (Drama, Action)\n",
      "7       1.0                            (Romance, Action)\n",
      "8       1.0                             (Sci-Fi, Action)\n",
      "9       1.0                           (Drama, Adventure)\n",
      "10      1.0                         (Romance, Adventure)\n",
      "11      1.0                          (Sci-Fi, Adventure)\n",
      "12      1.0                             (Drama, Romance)\n",
      "13      1.0                              (Drama, Sci-Fi)\n",
      "14      1.0                            (Romance, Sci-Fi)\n",
      "15      1.0                   (Drama, Action, Adventure)\n",
      "16      1.0                 (Romance, Action, Adventure)\n",
      "17      1.0                  (Sci-Fi, Action, Adventure)\n",
      "18      1.0                     (Drama, Romance, Action)\n",
      "19      1.0                      (Drama, Sci-Fi, Action)\n",
      "20      1.0                    (Romance, Sci-Fi, Action)\n",
      "21      1.0                  (Drama, Romance, Adventure)\n",
      "22      1.0                   (Drama, Sci-Fi, Adventure)\n",
      "23      1.0                 (Romance, Sci-Fi, Adventure)\n",
      "24      1.0                     (Drama, Romance, Sci-Fi)\n",
      "25      1.0          (Drama, Romance, Action, Adventure)\n",
      "26      1.0           (Drama, Sci-Fi, Action, Adventure)\n",
      "27      1.0         (Romance, Sci-Fi, Action, Adventure)\n",
      "28      1.0             (Drama, Romance, Sci-Fi, Action)\n",
      "29      1.0          (Drama, Romance, Sci-Fi, Adventure)\n",
      "30      1.0  (Sci-Fi, Adventure, Drama, Romance, Action)\n",
      "\n",
      "Association Rules:\n",
      "      antecedents                           consequents  antecedent support  \\\n",
      "0       (Action)                           (Adventure)                 1.0   \n",
      "1    (Adventure)                              (Action)                 1.0   \n",
      "2        (Drama)                              (Action)                 1.0   \n",
      "3       (Action)                               (Drama)                 1.0   \n",
      "4      (Romance)                              (Action)                 1.0   \n",
      "..           ...                                   ...                 ...   \n",
      "175     (Sci-Fi)   (Drama, Romance, Action, Adventure)                 1.0   \n",
      "176  (Adventure)      (Drama, Action, Romance, Sci-Fi)                 1.0   \n",
      "177      (Drama)  (Action, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "178    (Romance)    (Drama, Action, Sci-Fi, Adventure)                 1.0   \n",
      "179     (Action)   (Drama, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "\n",
      "     consequent support  support  confidence  lift  leverage  conviction  \\\n",
      "0                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "1                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "2                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "3                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "4                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "..                  ...      ...         ...   ...       ...         ...   \n",
      "175                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "176                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "177                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "178                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "179                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "\n",
      "     zhangs_metric  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "..             ...  \n",
      "175            0.0  \n",
      "176            0.0  \n",
      "177            0.0  \n",
      "178            0.0  \n",
      "179            0.0  \n",
      "\n",
      "[180 rows x 10 columns]\n",
      "\n",
      "\n",
      "Results for 25 transactions:\n",
      "Frequent Itemsets:\n",
      "     support                                     itemsets\n",
      "0       1.0                                     (Action)\n",
      "1       1.0                                  (Adventure)\n",
      "2       1.0                                      (Drama)\n",
      "3       1.0                                    (Romance)\n",
      "4       1.0                                     (Sci-Fi)\n",
      "5       1.0                          (Action, Adventure)\n",
      "6       1.0                              (Drama, Action)\n",
      "7       1.0                            (Romance, Action)\n",
      "8       1.0                             (Sci-Fi, Action)\n",
      "9       1.0                           (Drama, Adventure)\n",
      "10      1.0                         (Romance, Adventure)\n",
      "11      1.0                          (Sci-Fi, Adventure)\n",
      "12      1.0                             (Drama, Romance)\n",
      "13      1.0                              (Drama, Sci-Fi)\n",
      "14      1.0                            (Romance, Sci-Fi)\n",
      "15      1.0                   (Drama, Action, Adventure)\n",
      "16      1.0                 (Romance, Action, Adventure)\n",
      "17      1.0                  (Sci-Fi, Action, Adventure)\n",
      "18      1.0                     (Drama, Romance, Action)\n",
      "19      1.0                      (Drama, Sci-Fi, Action)\n",
      "20      1.0                    (Romance, Sci-Fi, Action)\n",
      "21      1.0                  (Drama, Romance, Adventure)\n",
      "22      1.0                   (Drama, Sci-Fi, Adventure)\n",
      "23      1.0                 (Romance, Sci-Fi, Adventure)\n",
      "24      1.0                     (Drama, Romance, Sci-Fi)\n",
      "25      1.0          (Drama, Romance, Action, Adventure)\n",
      "26      1.0           (Drama, Sci-Fi, Action, Adventure)\n",
      "27      1.0         (Romance, Sci-Fi, Action, Adventure)\n",
      "28      1.0             (Drama, Romance, Sci-Fi, Action)\n",
      "29      1.0          (Drama, Romance, Sci-Fi, Adventure)\n",
      "30      1.0  (Sci-Fi, Adventure, Drama, Romance, Action)\n",
      "\n",
      "Association Rules:\n",
      "      antecedents                           consequents  antecedent support  \\\n",
      "0       (Action)                           (Adventure)                 1.0   \n",
      "1    (Adventure)                              (Action)                 1.0   \n",
      "2        (Drama)                              (Action)                 1.0   \n",
      "3       (Action)                               (Drama)                 1.0   \n",
      "4      (Romance)                              (Action)                 1.0   \n",
      "..           ...                                   ...                 ...   \n",
      "175     (Sci-Fi)   (Drama, Romance, Action, Adventure)                 1.0   \n",
      "176  (Adventure)      (Drama, Action, Romance, Sci-Fi)                 1.0   \n",
      "177      (Drama)  (Action, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "178    (Romance)    (Drama, Action, Sci-Fi, Adventure)                 1.0   \n",
      "179     (Action)   (Drama, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "\n",
      "     consequent support  support  confidence  lift  leverage  conviction  \\\n",
      "0                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "1                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "2                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "3                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "4                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "..                  ...      ...         ...   ...       ...         ...   \n",
      "175                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "176                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "177                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "178                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "179                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "\n",
      "     zhangs_metric  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "..             ...  \n",
      "175            0.0  \n",
      "176            0.0  \n",
      "177            0.0  \n",
      "178            0.0  \n",
      "179            0.0  \n",
      "\n",
      "[180 rows x 10 columns]\n",
      "\n",
      "\n",
      "Results for 30 transactions:\n",
      "Frequent Itemsets:\n",
      "     support                                     itemsets\n",
      "0       1.0                                     (Action)\n",
      "1       1.0                                  (Adventure)\n",
      "2       1.0                                      (Drama)\n",
      "3       1.0                                    (Romance)\n",
      "4       1.0                                     (Sci-Fi)\n",
      "5       1.0                          (Action, Adventure)\n",
      "6       1.0                              (Drama, Action)\n",
      "7       1.0                            (Romance, Action)\n",
      "8       1.0                             (Sci-Fi, Action)\n",
      "9       1.0                           (Drama, Adventure)\n",
      "10      1.0                         (Romance, Adventure)\n",
      "11      1.0                          (Sci-Fi, Adventure)\n",
      "12      1.0                             (Drama, Romance)\n",
      "13      1.0                              (Drama, Sci-Fi)\n",
      "14      1.0                            (Romance, Sci-Fi)\n",
      "15      1.0                   (Drama, Action, Adventure)\n",
      "16      1.0                 (Romance, Action, Adventure)\n",
      "17      1.0                  (Sci-Fi, Action, Adventure)\n",
      "18      1.0                     (Drama, Romance, Action)\n",
      "19      1.0                      (Drama, Sci-Fi, Action)\n",
      "20      1.0                    (Romance, Sci-Fi, Action)\n",
      "21      1.0                  (Drama, Romance, Adventure)\n",
      "22      1.0                   (Drama, Sci-Fi, Adventure)\n",
      "23      1.0                 (Romance, Sci-Fi, Adventure)\n",
      "24      1.0                     (Drama, Romance, Sci-Fi)\n",
      "25      1.0          (Drama, Romance, Action, Adventure)\n",
      "26      1.0           (Drama, Sci-Fi, Action, Adventure)\n",
      "27      1.0         (Romance, Sci-Fi, Action, Adventure)\n",
      "28      1.0             (Drama, Romance, Sci-Fi, Action)\n",
      "29      1.0          (Drama, Romance, Sci-Fi, Adventure)\n",
      "30      1.0  (Sci-Fi, Adventure, Drama, Romance, Action)\n",
      "\n",
      "Association Rules:\n",
      "      antecedents                           consequents  antecedent support  \\\n",
      "0       (Action)                           (Adventure)                 1.0   \n",
      "1    (Adventure)                              (Action)                 1.0   \n",
      "2        (Drama)                              (Action)                 1.0   \n",
      "3       (Action)                               (Drama)                 1.0   \n",
      "4      (Romance)                              (Action)                 1.0   \n",
      "..           ...                                   ...                 ...   \n",
      "175     (Sci-Fi)   (Drama, Romance, Action, Adventure)                 1.0   \n",
      "176  (Adventure)      (Drama, Action, Romance, Sci-Fi)                 1.0   \n",
      "177      (Drama)  (Action, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "178    (Romance)    (Drama, Action, Sci-Fi, Adventure)                 1.0   \n",
      "179     (Action)   (Drama, Romance, Sci-Fi, Adventure)                 1.0   \n",
      "\n",
      "     consequent support  support  confidence  lift  leverage  conviction  \\\n",
      "0                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "1                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "2                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "3                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "4                   1.0      1.0         1.0   1.0       0.0         inf   \n",
      "..                  ...      ...         ...   ...       ...         ...   \n",
      "175                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "176                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "177                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "178                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "179                 1.0      1.0         1.0   1.0       0.0         inf   \n",
      "\n",
      "     zhangs_metric  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "..             ...  \n",
      "175            0.0  \n",
      "176            0.0  \n",
      "177            0.0  \n",
      "178            0.0  \n",
      "179            0.0  \n",
      "\n",
      "[180 rows x 10 columns]\n",
      "\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Generating random transactions for the same set of items\n",
    "num_transactions_list = [10, 15, 20, 25, 30]  # Varying number of transactions\n",
    "items = ['Action', 'Adventure', 'Drama', 'Romance', 'Sci-Fi']  # Same set of unique items\n",
    "\n",
    "data = []\n",
    "for num_transactions in num_transactions_list:\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        random.shuffle(items)\n",
    "        transactions.append(items.copy())\n",
    "    data.append(transactions)\n",
    "\n",
    "# Convert the dataset to a one-hot encoded DataFrame\n",
    "te = TransactionEncoder()\n",
    "te_ary_list = [te.fit_transform(transactions) for transactions in data]\n",
    "df_list = [pd.DataFrame(te_ary, columns=te.columns_) for te_ary in te_ary_list]\n",
    "\n",
    "# Applying the Apriori algorithm for each dataset\n",
    "for idx, df in enumerate(df_list):\n",
    "    print(f\"Results for {num_transactions_list[idx]} transactions:\")\n",
    "    frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "    print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
    "    print(\"\\nAssociation Rules:\\n\", rules)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeefd4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "('Action',): 8\n",
      "('Adventure',): 4\n",
      "('Sci-Fi',): 5\n",
      "('Action', 'Adventure'): 4\n",
      "('Action', 'Sci-Fi'): 4\n",
      "('Drama',): 10\n",
      "('Romance',): 9\n",
      "('Action', 'Drama'): 3\n",
      "('Action', 'Romance'): 3\n",
      "('Drama', 'Romance'): 7\n",
      "('Drama', 'Sci-Fi'): 3\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 212 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    ['Action', 'Adventure', 'Sci-Fi'],\n",
    "    ['Action', 'Drama', 'Romance'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Adventure', 'Drama', 'Sci-Fi'],\n",
    "    ['Action', 'Romance'],\n",
    "    ['Drama', 'Sci-Fi'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Adventure', 'Romance'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Drama', 'Sci-Fi'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Sci-Fi'],\n",
    "    ['Drama', 'Romance'],\n",
    "    ['Action', 'Adventure'],\n",
    "    ['Drama', 'Romance']\n",
    "]\n",
    "\n",
    "# Function to generate all possible itemsets\n",
    "def get_all_itemsets(data):\n",
    "    itemsets = defaultdict(int)\n",
    "    for row in data:\n",
    "        for i in range(1, len(row) + 1):\n",
    "            for subset in combinations(row, i):\n",
    "                itemsets[subset] += 1\n",
    "    return itemsets\n",
    "\n",
    "# Function to filter frequent itemsets based on minimum support\n",
    "def filter_itemsets(itemsets, min_support):\n",
    "    num_items = len(dataset)\n",
    "    return {itemset: support for itemset, support in itemsets.items() if support / num_items >= min_support}\n",
    "\n",
    "# Setting the minimum support\n",
    "min_support = 0.2\n",
    "\n",
    "# Getting all possible itemsets\n",
    "all_itemsets = get_all_itemsets(dataset)\n",
    "\n",
    "# Filtering frequent itemsets based on the minimum support\n",
    "frequent_itemsets = filter_itemsets(all_itemsets, min_support)\n",
    "\n",
    "# Printing the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset, support in frequent_itemsets.items():\n",
    "    print(f\"{itemset}: {support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fad44888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "('Milk',): 9\n",
      "('Bread',): 12\n",
      "('Butter',): 6\n",
      "('Milk', 'Bread'): 6\n",
      "('Milk', 'Butter'): 3\n",
      "('Bread', 'Butter'): 6\n",
      "('Milk', 'Bread', 'Butter'): 3\n",
      "('Juice',): 7\n",
      "('Milk', 'Juice'): 4\n",
      "('Bread', 'Juice'): 4\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample dataset\n",
    "data_market = [['Milk', 'Bread', 'Butter'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Milk', 'Bread', 'Butter'],\n",
    "        ['Milk', 'Bread'],\n",
    "        ['Milk', 'Juice'],\n",
    "        ['Bread', 'Juice'],\n",
    "        ['Milk', 'Bread', 'Juice'],\n",
    "        ['Milk', 'Bread', 'Butter'],\n",
    "        ['Milk', 'Juice'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Milk', 'Juice'],\n",
    "        ['Bread', 'Juice'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Milk', 'Bread'],\n",
    "        ['Bread', 'Juice']]\n",
    "\n",
    "# Function to generate all possible itemsets\n",
    "def get_all_itemsets(data_market):\n",
    "    itemsets = defaultdict(int)\n",
    "    for row in data_market:\n",
    "        for i in range(1, len(row) + 1):\n",
    "            for subset in combinations(row, i):\n",
    "                itemsets[subset] += 1\n",
    "    return itemsets\n",
    "\n",
    "# Function to filter frequent itemsets based on minimum support\n",
    "def filter_itemsets(itemsets, min_support):\n",
    "    num_items = len(dataset)\n",
    "    return {itemset: support for itemset, support in itemsets.items() if support / num_items >= min_support}\n",
    "\n",
    "# Setting the minimum support\n",
    "min_support = 0.2\n",
    "\n",
    "# Getting all possible itemsets\n",
    "all_itemsets = get_all_itemsets(data_market)\n",
    "\n",
    "# Filtering frequent itemsets based on the minimum support\n",
    "frequent_itemsets = filter_itemsets(all_itemsets, min_support)\n",
    "\n",
    "# Printing the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset, support in frequent_itemsets.items():\n",
    "    print(f\"{itemset}: {support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfa2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
