{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76992863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggleNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "     ---------------------------------------- 0.0/83.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.6/83.6 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: bleach in c:\\users\\samue\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\samue\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\samue\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\samue\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110697 sha256=a7c50c51df5be5c26ad24da7fd188089c00721d754628e2232d2d89fbc1cb4dd\n",
      "  Stored in directory: c:\\users\\samue\\appdata\\local\\pip\\cache\\wheels\\6a\\2b\\d0\\457dd27de499e9423caf738e743c4a3f82886ee6b19f89d5b7\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.16\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb512111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support                                           itemsets\n",
      "0       0.1                                     (citrus fruit)\n",
      "1       0.1                                        (margarine)\n",
      "2       0.1                                      (ready soups)\n",
      "3       0.1                              (semi-finished bread)\n",
      "4       0.3                                           (yogurt)\n",
      "..      ...                                                ...\n",
      "92      0.1         (bottled beer, UHT-milk, other vegetables)\n",
      "93      0.1               (bottled beer, rolls/buns, UHT-milk)\n",
      "94      0.1           (rolls/buns, UHT-milk, other vegetables)\n",
      "95      0.1  (bottled beer, rolls/buns, UHT-milk, other veg...\n",
      "96      0.1                              (whole milk, cereals)\n",
      "\n",
      "[97 rows x 2 columns]\n",
      "CPU times: total: 1 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Sample groceries dataset\n",
    "data = [['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups'],\n",
    "        ['tropical fruit', 'yogurt', 'coffee'],\n",
    "        ['whole milk'],\n",
    "        ['pip fruit', 'yogurt', 'cream cheese', 'meat spreads'],\n",
    "        ['other vegetables', 'whole milk', 'condensed milk', 'long life bakery product'],\n",
    "        ['whole milk', 'butter', 'yogurt', 'rice', 'abrasive cleaner'],\n",
    "        ['rolls/buns'],\n",
    "        ['other vegetables', 'UHT-milk', 'rolls/buns', 'bottled beer'],\n",
    "        ['pot plants'],\n",
    "        ['whole milk', 'cereals']]\n",
    "\n",
    "# Convert the dataset into a one-hot encoded NumPy array\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply FP-growth algorithm\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a980b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent itemsets for dataset 1:\n",
      "   support               itemsets\n",
      "0     0.75                (item3)\n",
      "1     0.75                (item2)\n",
      "2     0.75                (item1)\n",
      "3     0.50         (item3, item2)\n",
      "4     0.50         (item2, item1)\n",
      "5     0.50         (item3, item1)\n",
      "6     0.25  (item3, item2, item1)\n",
      "\n",
      "Frequent itemsets for dataset 2:\n",
      "   support               itemsets\n",
      "0      0.8                (item3)\n",
      "1      0.8                (item2)\n",
      "2      0.8                (item1)\n",
      "3      0.6         (item3, item2)\n",
      "4      0.6         (item2, item1)\n",
      "5      0.6         (item3, item1)\n",
      "6      0.4  (item3, item2, item1)\n",
      "\n",
      "Frequent itemsets for dataset 3:\n",
      "    support               itemsets\n",
      "0  0.833333                (item2)\n",
      "1  0.833333                (item1)\n",
      "2  0.666667                (item3)\n",
      "3  0.666667         (item2, item1)\n",
      "4  0.500000         (item3, item1)\n",
      "5  0.500000         (item3, item2)\n",
      "6  0.333333  (item3, item2, item1)\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 9.62 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Varying number of transactions with the same number of unique items\n",
    "dataset_1 = [['item1', 'item2', 'item3'],\n",
    "             ['item1', 'item2'],\n",
    "             ['item1', 'item3'],\n",
    "             ['item2', 'item3']]\n",
    "\n",
    "dataset_2 = [['item1', 'item2', 'item3'],\n",
    "             ['item1', 'item2'],\n",
    "             ['item1', 'item3'],\n",
    "             ['item2', 'item3'],\n",
    "             ['item1', 'item2', 'item3']]\n",
    "\n",
    "dataset_3 = [['item1', 'item2', 'item3'],\n",
    "             ['item1', 'item2'],\n",
    "             ['item1', 'item3'],\n",
    "             ['item2', 'item3'],\n",
    "             ['item1', 'item2', 'item3'],\n",
    "             ['item1', 'item2']]\n",
    "\n",
    "# Convert the datasets into one-hot encoded NumPy arrays\n",
    "te = TransactionEncoder()\n",
    "te_ary_1 = te.fit(dataset_1).transform(dataset_1)\n",
    "df_1 = pd.DataFrame(te_ary_1, columns=te.columns_)\n",
    "\n",
    "te_ary_2 = te.fit(dataset_2).transform(dataset_2)\n",
    "df_2 = pd.DataFrame(te_ary_2, columns=te.columns_)\n",
    "\n",
    "te_ary_3 = te.fit(dataset_3).transform(dataset_3)\n",
    "df_3 = pd.DataFrame(te_ary_3, columns=te.columns_)\n",
    "\n",
    "# Apply FP-growth algorithm\n",
    "frequent_itemsets_1 = fpgrowth(df_1, min_support=0.1, use_colnames=True)\n",
    "frequent_itemsets_2 = fpgrowth(df_2, min_support=0.1, use_colnames=True)\n",
    "frequent_itemsets_3 = fpgrowth(df_3, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Frequent itemsets for dataset 1:\")\n",
    "print(frequent_itemsets_1)\n",
    "print(\"\\nFrequent itemsets for dataset 2:\")\n",
    "print(frequent_itemsets_2)\n",
    "print(\"\\nFrequent itemsets for dataset 3:\")\n",
    "print(frequent_itemsets_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecb20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support                                           itemsets\n",
      "0       0.1                                         (UHT-milk)\n",
      "1       0.1                                 (abrasive cleaner)\n",
      "2       0.1                                     (bottled beer)\n",
      "3       0.1                                           (butter)\n",
      "4       0.1                                          (cereals)\n",
      "..      ...                                                ...\n",
      "92      0.1                 (butter, yogurt, whole milk, rice)\n",
      "93      0.1  (semi-finished bread, citrus fruit, ready soup...\n",
      "94      0.1  (other vegetables, condensed milk, whole milk,...\n",
      "95      0.1    (cream cheese, yogurt, meat spreads, pip fruit)\n",
      "96      0.1  (butter, yogurt, abrasive cleaner, whole milk,...\n",
      "\n",
      "[97 rows x 2 columns]\n",
      "            antecedents                                     consequents  \\\n",
      "0        (bottled beer)                                      (UHT-milk)   \n",
      "1            (UHT-milk)                                  (bottled beer)   \n",
      "2            (UHT-milk)                              (other vegetables)   \n",
      "3    (other vegetables)                                      (UHT-milk)   \n",
      "4          (rolls/buns)                                      (UHT-milk)   \n",
      "..                  ...                                             ...   \n",
      "387            (butter)    (yogurt, rice, whole milk, abrasive cleaner)   \n",
      "388            (yogurt)    (butter, rice, whole milk, abrasive cleaner)   \n",
      "389  (abrasive cleaner)              (butter, yogurt, rice, whole milk)   \n",
      "390        (whole milk)        (butter, yogurt, rice, abrasive cleaner)   \n",
      "391              (rice)  (butter, yogurt, whole milk, abrasive cleaner)   \n",
      "\n",
      "     antecedent support  consequent support  support  confidence       lift  \\\n",
      "0                   0.1                 0.1      0.1    1.000000  10.000000   \n",
      "1                   0.1                 0.1      0.1    1.000000  10.000000   \n",
      "2                   0.1                 0.2      0.1    1.000000   5.000000   \n",
      "3                   0.2                 0.1      0.1    0.500000   5.000000   \n",
      "4                   0.2                 0.1      0.1    0.500000   5.000000   \n",
      "..                  ...                 ...      ...         ...        ...   \n",
      "387                 0.1                 0.1      0.1    1.000000  10.000000   \n",
      "388                 0.3                 0.1      0.1    0.333333   3.333333   \n",
      "389                 0.1                 0.1      0.1    1.000000  10.000000   \n",
      "390                 0.4                 0.1      0.1    0.250000   2.500000   \n",
      "391                 0.1                 0.1      0.1    1.000000  10.000000   \n",
      "\n",
      "     leverage  conviction  zhangs_metric  \n",
      "0        0.09         inf       1.000000  \n",
      "1        0.09         inf       1.000000  \n",
      "2        0.08         inf       0.888889  \n",
      "3        0.08        1.80       1.000000  \n",
      "4        0.08        1.80       1.000000  \n",
      "..        ...         ...            ...  \n",
      "387      0.09         inf       1.000000  \n",
      "388      0.07        1.35       1.000000  \n",
      "389      0.09         inf       1.000000  \n",
      "390      0.06        1.20       1.000000  \n",
      "391      0.09         inf       1.000000  \n",
      "\n",
      "[392 rows x 10 columns]\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 45.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Sample groceries dataset\n",
    "data = [['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups'],\n",
    "        ['tropical fruit', 'yogurt', 'coffee'],\n",
    "        ['whole milk'],\n",
    "        ['pip fruit', 'yogurt', 'cream cheese', 'meat spreads'],\n",
    "        ['other vegetables', 'whole milk', 'condensed milk', 'long life bakery product'],\n",
    "        ['whole milk', 'butter', 'yogurt', 'rice', 'abrasive cleaner'],\n",
    "        ['rolls/buns'],\n",
    "        ['other vegetables', 'UHT-milk', 'rolls/buns', 'bottled beer'],\n",
    "        ['pot plants'],\n",
    "        ['whole milk', 'cereals']]\n",
    "\n",
    "# Convert the dataset into a one-hot encoded NumPy array\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Print frequent itemsets\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e866e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support                      itemsets\n",
      "0       0.9                       (Bread)\n",
      "1       0.5                        (Milk)\n",
      "2       0.4                      (Butter)\n",
      "3       0.3                         (Jam)\n",
      "4       0.3                     (Cookies)\n",
      "5       0.3                      (Coffee)\n",
      "6       0.4                        (Cake)\n",
      "7       0.4                 (Milk, Bread)\n",
      "8       0.4               (Butter, Bread)\n",
      "9       0.2                (Milk, Butter)\n",
      "10      0.1                (Cake, Butter)\n",
      "11      0.2         (Milk, Butter, Bread)\n",
      "12      0.1         (Cake, Butter, Bread)\n",
      "13      0.1          (Cake, Butter, Milk)\n",
      "14      0.1   (Cake, Butter, Milk, Bread)\n",
      "15      0.3                  (Jam, Bread)\n",
      "16      0.1                 (Butter, Jam)\n",
      "17      0.1                   (Cake, Jam)\n",
      "18      0.1          (Jam, Butter, Bread)\n",
      "19      0.1            (Jam, Cake, Bread)\n",
      "20      0.2              (Cookies, Bread)\n",
      "21      0.2               (Cake, Cookies)\n",
      "22      0.2               (Milk, Cookies)\n",
      "23      0.1        (Milk, Cookies, Bread)\n",
      "24      0.1        (Cake, Cookies, Bread)\n",
      "25      0.1  (Cake, Cookies, Milk, Bread)\n",
      "26      0.2         (Cake, Cookies, Milk)\n",
      "27      0.3               (Coffee, Bread)\n",
      "28      0.1             (Cookies, Coffee)\n",
      "29      0.1                (Milk, Coffee)\n",
      "30      0.1                 (Jam, Coffee)\n",
      "31      0.1      (Cookies, Coffee, Bread)\n",
      "32      0.1         (Milk, Coffee, Bread)\n",
      "33      0.1          (Jam, Coffee, Bread)\n",
      "34      0.3                  (Cake, Milk)\n",
      "35      0.3                 (Cake, Bread)\n",
      "36      0.2           (Cake, Milk, Bread)\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 16.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Sample Bakery dataset\n",
    "data = [['Bread', 'Butter', 'Milk'],\n",
    "        ['Bread', 'Jam', 'Butter'],\n",
    "        ['Bread', 'Coffee', 'Cookies'],\n",
    "        ['Cake', 'Bread', 'Butter', 'Milk'],\n",
    "        ['Cake', 'Bread', 'Cookies', 'Milk'],\n",
    "        ['Bread', 'Coffee', 'Milk'],\n",
    "        ['Cake', 'Bread', 'Jam'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Cake', 'Cookies', 'Milk'],\n",
    "        ['Bread', 'Coffee', 'Jam']]\n",
    "\n",
    "# Convert the dataset into a one-hot encoded NumPy array\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply FP-growth algorithm\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc54c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate transactions with varying unique items\n",
    "def generate_transactions(num_transactions, max_items):\n",
    "    items = ['Item_' + str(i) for i in range(1, max_items + 1)]\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        num_items = random.randint(1, max_items)\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Generate transactions with varying unique items\n",
    "num_transactions = 15 # same as the previous example\n",
    "max_items = 50  # change the number of unique items here\n",
    "data_varying = generate_transactions(num_transactions, max_items)\n",
    "\n",
    "# Rest of the code remains the same as the previous example\n",
    "te_varying = TransactionEncoder()\n",
    "te_ary_varying = te_varying.fit(data_varying).transform(data_varying)\n",
    "df_varying = pd.DataFrame(te_ary_varying, columns=te_varying.columns_)\n",
    "\n",
    "frequent_itemsets_varying = apriori(df_varying, min_support=0.2, use_colnames=True)\n",
    "rules_varying = association_rules(frequent_itemsets_varying, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets with Varying Unique Items: \")\n",
    "print(frequent_itemsets_varying)\n",
    "print(\"\\nAssociation Rules with Varying Unique Items: \")\n",
    "print(rules_varying)\n",
    "#By changing the value of max_items, you can generate a dataset with a different number of unique items. This modified code will still utilize the mlxtend library and produce the frequent itemsets and association rules for the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a59199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets with Varying Unique Items: \n",
      "       support                                           itemsets\n",
      "0     0.200000                                           (Item_1)\n",
      "1     0.400000                                          (Item_10)\n",
      "2     0.466667                                          (Item_11)\n",
      "3     0.400000                                          (Item_12)\n",
      "4     0.533333                                          (Item_13)\n",
      "...        ...                                                ...\n",
      "7568  0.200000  (Item_12, Item_22, Item_29, Item_2, Item_20, I...\n",
      "7569  0.200000  (Item_4, Item_12, Item_22, Item_29, Item_20, I...\n",
      "7570  0.200000  (Item_12, Item_22, Item_29, Item_30, Item_20, ...\n",
      "7571  0.200000  (Item_12, Item_22, Item_29, Item_3, Item_23, I...\n",
      "7572  0.200000  (Item_4, Item_12, Item_22, Item_29, Item_10, I...\n",
      "\n",
      "[7573 rows x 2 columns]\n",
      "\n",
      "Association Rules with Varying Unique Items: \n",
      "               antecedents                                        consequents  \\\n",
      "0                 (Item_1)                                          (Item_10)   \n",
      "1                 (Item_1)                                          (Item_13)   \n",
      "2                 (Item_1)                                          (Item_20)   \n",
      "3                (Item_10)                                          (Item_11)   \n",
      "4                (Item_10)                                          (Item_12)   \n",
      "...                    ...                                                ...   \n",
      "397074  (Item_13, Item_27)  (Item_4, Item_12, Item_29, Item_10, Item_2, It...   \n",
      "397075  (Item_13, Item_17)  (Item_4, Item_12, Item_29, Item_10, Item_2, It...   \n",
      "397076  (Item_27, Item_17)  (Item_4, Item_12, Item_29, Item_13, Item_10, I...   \n",
      "397077   (Item_27, Item_9)  (Item_4, Item_12, Item_29, Item_13, Item_10, I...   \n",
      "397078   (Item_17, Item_9)  (Item_4, Item_12, Item_29, Item_13, Item_10, I...   \n",
      "\n",
      "        antecedent support  consequent support   support  confidence  \\\n",
      "0                 0.200000            0.400000  0.200000    1.000000   \n",
      "1                 0.200000            0.533333  0.200000    1.000000   \n",
      "2                 0.200000            0.466667  0.200000    1.000000   \n",
      "3                 0.400000            0.466667  0.266667    0.666667   \n",
      "4                 0.400000            0.400000  0.266667    0.666667   \n",
      "...                    ...                 ...       ...         ...   \n",
      "397074            0.333333            0.200000  0.200000    0.600000   \n",
      "397075            0.266667            0.200000  0.200000    0.750000   \n",
      "397076            0.333333            0.200000  0.200000    0.600000   \n",
      "397077            0.266667            0.200000  0.200000    0.750000   \n",
      "397078            0.266667            0.200000  0.200000    0.750000   \n",
      "\n",
      "            lift  leverage  conviction  zhangs_metric  \n",
      "0       2.500000  0.120000         inf       0.750000  \n",
      "1       1.875000  0.093333         inf       0.583333  \n",
      "2       2.142857  0.106667         inf       0.666667  \n",
      "3       1.428571  0.080000         1.6       0.500000  \n",
      "4       1.666667  0.106667         1.8       0.666667  \n",
      "...          ...       ...         ...            ...  \n",
      "397074  3.000000  0.133333         2.0       1.000000  \n",
      "397075  3.750000  0.146667         3.2       1.000000  \n",
      "397076  3.000000  0.133333         2.0       1.000000  \n",
      "397077  3.750000  0.146667         3.2       1.000000  \n",
      "397078  3.750000  0.146667         3.2       1.000000  \n",
      "\n",
      "[397079 rows x 10 columns]\n",
      "CPU times: total: 1.67 s\n",
      "Wall time: 3.81 s\n",
      "CPU times: total: 1.67 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate transactions with varying unique items\n",
    "def generate_transactions(num_transactions, max_items):\n",
    "    items = ['Item_' + str(i) for i in range(1, max_items + 1)]\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        num_items = random.randint(1, max_items)\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Generate transactions with varying unique items\n",
    "num_transactions = 15  # same as the previous example\n",
    "max_items = 30  # change the number of unique items here\n",
    "data_varying = generate_transactions(num_transactions, max_items)\n",
    "\n",
    "# Rest of the code remains the same as the previous example\n",
    "te_varying = TransactionEncoder()\n",
    "te_ary_varying = te_varying.fit(data_varying).transform(data_varying)\n",
    "df_varying = pd.DataFrame(te_ary_varying, columns=te_varying.columns_)\n",
    "\n",
    "frequent_itemsets_varying = apriori(df_varying, min_support=0.2, use_colnames=True)\n",
    "rules_varying = association_rules(frequent_itemsets_varying, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets with Varying Unique Items: \")\n",
    "print(frequent_itemsets_varying)\n",
    "print(\"\\nAssociation Rules with Varying Unique Items: \")\n",
    "print(rules_varying)\n",
    "#By changing the value of max_items, you can generate a dataset with a different number of unique items. This modified code will still utilize the mlxtend library and produce the frequent itemsets and association rules for the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b773d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets with Varying Unique Items: \n",
      "      support                                           itemsets\n",
      "0    0.533333                                           (Item_1)\n",
      "1    0.466667                                          (Item_10)\n",
      "2    0.533333                                           (Item_2)\n",
      "3    0.533333                                           (Item_3)\n",
      "4    0.533333                                           (Item_4)\n",
      "..        ...                                                ...\n",
      "794  0.200000  (Item_1, Item_6, Item_3, Item_5, Item_2, Item_...\n",
      "795  0.200000  (Item_4, Item_10, Item_3, Item_5, Item_2, Item...\n",
      "796  0.200000  (Item_10, Item_3, Item_6, Item_2, Item_5, Item...\n",
      "797  0.200000  (Item_4, Item_1, Item_10, Item_3, Item_5, Item...\n",
      "798  0.200000  (Item_1, Item_10, Item_3, Item_5, Item_2, Item...\n",
      "\n",
      "[799 rows x 2 columns]\n",
      "\n",
      "Association Rules with Varying Unique Items: \n",
      "            antecedents                                        consequents  \\\n",
      "0             (Item_10)                                           (Item_1)   \n",
      "1              (Item_1)                                          (Item_10)   \n",
      "2              (Item_1)                                           (Item_2)   \n",
      "3              (Item_2)                                           (Item_1)   \n",
      "4              (Item_3)                                           (Item_1)   \n",
      "...                 ...                                                ...   \n",
      "23952  (Item_7, Item_2)  (Item_1, Item_10, Item_3, Item_6, Item_5, Item...   \n",
      "23953  (Item_6, Item_7)  (Item_1, Item_10, Item_3, Item_5, Item_2, Item...   \n",
      "23954  (Item_8, Item_6)  (Item_1, Item_10, Item_3, Item_5, Item_2, Item...   \n",
      "23955  (Item_6, Item_9)  (Item_1, Item_10, Item_3, Item_5, Item_2, Item...   \n",
      "23956          (Item_6)  (Item_1, Item_10, Item_3, Item_5, Item_2, Item...   \n",
      "\n",
      "       antecedent support  consequent support   support  confidence      lift  \\\n",
      "0                0.466667            0.533333  0.400000    0.857143  1.607143   \n",
      "1                0.533333            0.466667  0.400000    0.750000  1.607143   \n",
      "2                0.533333            0.533333  0.466667    0.875000  1.640625   \n",
      "3                0.533333            0.533333  0.466667    0.875000  1.640625   \n",
      "4                0.533333            0.533333  0.333333    0.625000  1.171875   \n",
      "...                   ...                 ...       ...         ...       ...   \n",
      "23952            0.333333            0.200000  0.200000    0.600000  3.000000   \n",
      "23953            0.266667            0.266667  0.200000    0.750000  2.812500   \n",
      "23954            0.266667            0.266667  0.200000    0.750000  2.812500   \n",
      "23955            0.266667            0.266667  0.200000    0.750000  2.812500   \n",
      "23956            0.333333            0.266667  0.200000    0.600000  2.250000   \n",
      "\n",
      "       leverage  conviction  zhangs_metric  \n",
      "0      0.151111    3.266667       0.708333  \n",
      "1      0.151111    2.133333       0.809524  \n",
      "2      0.182222    3.733333       0.836735  \n",
      "3      0.182222    3.733333       0.836735  \n",
      "4      0.048889    1.244444       0.314286  \n",
      "...         ...         ...            ...  \n",
      "23952  0.133333    2.000000       1.000000  \n",
      "23953  0.128889    2.933333       0.878788  \n",
      "23954  0.128889    2.933333       0.878788  \n",
      "23955  0.128889    2.933333       0.878788  \n",
      "23956  0.111111    1.833333       0.833333  \n",
      "\n",
      "[23957 rows x 10 columns]\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 223 ms\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 223 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate transactions with varying unique items\n",
    "def generate_transactions(num_transactions, max_items):\n",
    "    items = ['Item_' + str(i) for i in range(1, max_items + 1)]\n",
    "    transactions = []\n",
    "    for _ in range(num_transactions):\n",
    "        num_items = random.randint(1, max_items)\n",
    "        transaction = random.sample(items, num_items)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Generate transactions with varying unique items\n",
    "num_transactions = 15  # same as the previous example\n",
    "max_items = 10  # change the number of unique items here\n",
    "data_varying = generate_transactions(num_transactions, max_items)\n",
    "\n",
    "# Rest of the code remains the same as the previous example\n",
    "te_varying = TransactionEncoder()\n",
    "te_ary_varying = te_varying.fit(data_varying).transform(data_varying)\n",
    "df_varying = pd.DataFrame(te_ary_varying, columns=te_varying.columns_)\n",
    "\n",
    "frequent_itemsets_varying = apriori(df_varying, min_support=0.2, use_colnames=True)\n",
    "rules_varying = association_rules(frequent_itemsets_varying, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "print(\"Frequent Itemsets with Varying Unique Items: \")\n",
    "print(frequent_itemsets_varying)\n",
    "print(\"\\nAssociation Rules with Varying Unique Items: \")\n",
    "print(rules_varying)\n",
    "#By changing the value of max_items, you can generate a dataset with a different number of unique items. This modified code will still utilize the mlxtend library and produce the frequent itemsets and association rules for the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ffbb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support                      itemsets\n",
      "0       0.9                       (Bread)\n",
      "1       0.4                      (Butter)\n",
      "2       0.4                        (Cake)\n",
      "3       0.3                      (Coffee)\n",
      "4       0.3                     (Cookies)\n",
      "5       0.3                         (Jam)\n",
      "6       0.5                        (Milk)\n",
      "7       0.4               (Butter, Bread)\n",
      "8       0.3                 (Cake, Bread)\n",
      "9       0.3               (Coffee, Bread)\n",
      "10      0.2              (Cookies, Bread)\n",
      "11      0.3                  (Jam, Bread)\n",
      "12      0.4                 (Milk, Bread)\n",
      "13      0.1                (Cake, Butter)\n",
      "14      0.1                 (Butter, Jam)\n",
      "15      0.2                (Milk, Butter)\n",
      "16      0.2               (Cake, Cookies)\n",
      "17      0.1                   (Cake, Jam)\n",
      "18      0.3                  (Cake, Milk)\n",
      "19      0.1             (Cookies, Coffee)\n",
      "20      0.1                 (Jam, Coffee)\n",
      "21      0.1                (Milk, Coffee)\n",
      "22      0.2               (Milk, Cookies)\n",
      "23      0.1         (Cake, Butter, Bread)\n",
      "24      0.1          (Jam, Butter, Bread)\n",
      "25      0.2         (Milk, Butter, Bread)\n",
      "26      0.1        (Cake, Cookies, Bread)\n",
      "27      0.1            (Jam, Cake, Bread)\n",
      "28      0.2           (Cake, Milk, Bread)\n",
      "29      0.1      (Cookies, Coffee, Bread)\n",
      "30      0.1          (Jam, Coffee, Bread)\n",
      "31      0.1         (Milk, Coffee, Bread)\n",
      "32      0.1        (Milk, Cookies, Bread)\n",
      "33      0.1          (Cake, Butter, Milk)\n",
      "34      0.2         (Cake, Cookies, Milk)\n",
      "35      0.1   (Cake, Butter, Milk, Bread)\n",
      "36      0.1  (Cake, Cookies, Milk, Bread)\n",
      "         antecedents             consequents  antecedent support  \\\n",
      "0           (Butter)                 (Bread)                 0.4   \n",
      "1            (Bread)                (Butter)                 0.9   \n",
      "2           (Coffee)                 (Bread)                 0.3   \n",
      "3            (Bread)                (Coffee)                 0.9   \n",
      "4              (Jam)                 (Bread)                 0.3   \n",
      "..               ...                     ...                 ...   \n",
      "85  (Cookies, Bread)            (Cake, Milk)                 0.2   \n",
      "86     (Milk, Bread)         (Cake, Cookies)                 0.4   \n",
      "87            (Cake)  (Milk, Cookies, Bread)                 0.4   \n",
      "88         (Cookies)     (Cake, Milk, Bread)                 0.3   \n",
      "89            (Milk)  (Cake, Cookies, Bread)                 0.5   \n",
      "\n",
      "    consequent support  support  confidence      lift  leverage  conviction  \\\n",
      "0                  0.9      0.4    1.000000  1.111111      0.04         inf   \n",
      "1                  0.4      0.4    0.444444  1.111111      0.04    1.080000   \n",
      "2                  0.9      0.3    1.000000  1.111111      0.03         inf   \n",
      "3                  0.3      0.3    0.333333  1.111111      0.03    1.050000   \n",
      "4                  0.9      0.3    1.000000  1.111111      0.03         inf   \n",
      "..                 ...      ...         ...       ...       ...         ...   \n",
      "85                 0.3      0.1    0.500000  1.666667      0.04    1.400000   \n",
      "86                 0.2      0.1    0.250000  1.250000      0.02    1.066667   \n",
      "87                 0.1      0.1    0.250000  2.500000      0.06    1.200000   \n",
      "88                 0.2      0.1    0.333333  1.666667      0.04    1.200000   \n",
      "89                 0.1      0.1    0.200000  2.000000      0.05    1.125000   \n",
      "\n",
      "    zhangs_metric  \n",
      "0        0.166667  \n",
      "1        1.000000  \n",
      "2        0.142857  \n",
      "3        1.000000  \n",
      "4        0.142857  \n",
      "..            ...  \n",
      "85       0.500000  \n",
      "86       0.333333  \n",
      "87       1.000000  \n",
      "88       0.571429  \n",
      "89       1.000000  \n",
      "\n",
      "[90 rows x 10 columns]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 13.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Sample Bakery dataset\n",
    "data = [['Bread', 'Butter', 'Milk'],\n",
    "        ['Bread', 'Jam', 'Butter'],\n",
    "        ['Bread', 'Coffee', 'Cookies'],\n",
    "        ['Cake', 'Bread', 'Butter', 'Milk'],\n",
    "        ['Cake', 'Bread', 'Cookies', 'Milk'],\n",
    "        ['Bread', 'Coffee', 'Milk'],\n",
    "        ['Cake', 'Bread', 'Jam'],\n",
    "        ['Bread', 'Butter'],\n",
    "        ['Cake', 'Cookies', 'Milk'],\n",
    "        ['Bread', 'Coffee', 'Jam']]\n",
    "\n",
    "# Convert the dataset into a one-hot encoded NumPy array\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data).transform(data)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Print frequent itemsets\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b75ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
